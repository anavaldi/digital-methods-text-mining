{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to the basics of analysing words. \n",
    "You'll learn how to preprocess and represent text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend of symbols:\n",
    "\n",
    "- ü§ì: Tips\n",
    "\n",
    "- ü§ñüìù: Your turn\n",
    "\n",
    "- ‚ùì: Question\n",
    "\n",
    "- üí´: Extra exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll learn how to transform words into vectors. Let's start with one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library **<tt> sklearn <tt>** has a function that transforms categorical features to one-hot vectors:\n",
    "    \n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()\n",
    "\n",
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()\n",
    "\n",
    "doc1_array = array(doc1)\n",
    "doc2_array = array(doc2)\n",
    "\n",
    "doc3 = doc1+doc2\n",
    "data = list(doc3)\n",
    "\n",
    "values = array(data)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will transform words to numbers based on its position. To do so, we will use the **<tt> LabelEncoder() <tt>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the order of words in the matrix\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the news dataset and calculate the onehot encoding of the first new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Word embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a word2vec model in Python is scarily easy with **<tt> gensim <tt>**.\n",
    "    \n",
    "üåç https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the word2vec model taking a look to the parameters:\n",
    "\n",
    "üåç https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will prepare the data. To do so, we need to transform every sentence in a list within a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()\n",
    "\n",
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = [doc1, doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(doc3, size=300, window=3, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can analyse the vocabulary of this word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the embeddings by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['pizza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word2vec, we can analyse similarities across words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(negative=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And relations between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('pizza', 'eat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì Note that this model doesn't contain a lot of text, so it doesn't make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a word2vec embedding with the news corpus and extract the top 10 most similar words of *ultraviolet*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help to prepare the input for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'reindeer', 'is', 'the', 'emblematic', 'christmas', 'animal', 'and,', 'while', 'not', 'exactly', 'magical,', 'it', 'is', 'among', 'the', 'best', 'adapted', 'to', 'snowy', 'conditions.for', 'a', 'start,', 'a', 'reindeer‚Äôs', 'feet', 'have', 'four', 'toes', 'with', 'dewclaws', 'that', 'spread', 'out', 'to', 'distribute', 'its', 'weight', 'like', 'snowshoes,', 'and', 'are', 'equipped', 'with', 'sharp', 'hooves', 'for', 'digging', 'in', 'snow.a', 'reindeer‚Äôs', 'nose', 'warms', 'the', 'air', 'on', 'its', 'way', 'to', 'the', 'lungs,', 'cooling', 'it', 'again', 'before', 'it', 'is', 'exhaled.', 'as', 'well', 'as', 'retaining', 'heat,', 'this', 'helps', 'prevent', 'water', 'from', 'being', 'lost', 'as', 'vapour.', 'this', 'is', 'why', 'reindeer', 'breath', 'does', 'not', 'steam', 'like', 'human', 'and', 'horse', 'breath.a', 'reindeer‚Äôs', 'thick', 'double-layered', 'coat', 'is', 'so', 'efficient', 'that', 'it', 'is', 'more', 'likely', 'to', 'overheat', 'than', 'get', 'too', 'cold,', 'especially', 'when', 'running.', 'when', 'this', 'happens,', 'reindeer', 'pant', 'like', 'dogs', 'to', 'cool', 'down,', 'bypassing', 'the', 'nasal', 'heat', 'exchanger.snowfields', 'may', 'be', 'featureless', 'to', 'human', 'eyes,', 'but', 'reindeer', 'are', 'sensitive', 'to', 'ultraviolet', 'light,', 'an', 'evolutionary', 'development', 'that', 'only', 'occurred', 'after', 'the', 'animals', 'moved', 'to', 'arctic', 'regions.', 'snow', 'reflects', 'ultraviolet,', 'so', 'this', 'ultravision', 'allows', 'reindeer', 'to', 'spot', 'anything', 'lying', 'on', 'it,', 'in', 'particular', 'lichen,', 'which', 'they', 'eat,', 'and', 'traces', 'of', 'urine', 'showing', 'where', 'other', 'reindeer', 'have', 'passed.but', 'while', 'reindeer', 'thrive', 'in', 'christmas-card', 'weather,', 'they', 'are', 'increasingly', 'challenged', 'by', 'climate', 'change', 'and', 'the', 'freeze-thaw', 'conditions', 'that', 'produce', 'poor', 'grazing.']\n"
     ]
    }
   ],
   "source": [
    "# This is a loop that iterates over the dataframe\n",
    "news_vec = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sent = row['corpus'].lower()\n",
    "    sent = sent.split()\n",
    "    news_vec.append(sent)    \n",
    " \n",
    "# Print the first element of the list:\n",
    "print(news_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí´ Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the most similiar word to *climate*.\n",
    "- Calculate the similarity between *climate* and *weather*.\n",
    "- Calculate the most similar word to *huamanitarian* + *climate* - *droguth*.\n",
    "\n",
    "Does make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Word preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate the examples we have seen previously in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of separate symbols by introducing extra white space is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"I've been 2 times to New York in 2011, but did not have the constitution for it. It DIDN'T appeal to me. I preferred Los Angeles.\"\n",
    "tokens = [[token.text for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', \"'ve\", 'been', '2', 'times', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.'], ['It', \"DIDN'T\", 'appeal', 'to', 'me', '.'], ['I', 'preferred', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its dictionary based (lemma) is called **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[token.lemma_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-PRON-', 'have', 'be', '2', 'time', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'do', 'not', 'have', 'the', 'constitution', 'for', '-PRON-', '.'], ['-PRON-', \"DIDN'T\", 'appeal', 'to', '-PRON-', '.'], ['-PRON-', 'prefer', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its stem is called **stemming**. \n",
    "\n",
    "This process is more radical than lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stems = [[stemmer.stem(token) for token in sentence] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 've', 'been', '2', 'time', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitut', 'for', 'it', '.'], ['it', \"didn't\", 'appeal', 'to', 'me', '.'], ['i', 'prefer', 'los', 'angel', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech** corresponds to the process of classifying words to its category: nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [[token.pos_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRON', 'AUX', 'AUX', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'NUM', 'PUNCT', 'CCONJ', 'AUX', 'PART', 'AUX', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'PROPN', 'VERB', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'VERB', 'PROPN', 'PROPN', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords** is the process of removing words that cannot be beneficial for the analysis, like determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [[token.text for token in sentence if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV'} and not token.is_stop]\n",
    "for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['times', 'New', 'York', 'constitution'], [\"DIDN'T\", 'appeal'], ['preferred', 'Los', 'Angeles']]\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative using **<tt> nltk <tt>** is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing** is the process of classifying words in a sentence based on its syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'been', 'nsubj'),\n",
       "  (\"'ve\", 'been', 'aux'),\n",
       "  ('been', 'been', 'ROOT'),\n",
       "  ('2', 'been', 'attr'),\n",
       "  ('times', '2', 'quantmod'),\n",
       "  ('to', 'been', 'prep'),\n",
       "  ('New', 'York', 'compound'),\n",
       "  ('York', 'to', 'pobj'),\n",
       "  ('in', 'been', 'prep'),\n",
       "  ('2011', 'in', 'pobj'),\n",
       "  (',', 'been', 'punct'),\n",
       "  ('but', 'been', 'cc'),\n",
       "  ('did', 'have', 'aux'),\n",
       "  ('not', 'have', 'neg'),\n",
       "  ('have', 'been', 'conj'),\n",
       "  ('the', 'constitution', 'det'),\n",
       "  ('constitution', 'have', 'dobj'),\n",
       "  ('for', 'constitution', 'prep'),\n",
       "  ('it', 'for', 'pobj'),\n",
       "  ('.', 'been', 'punct')],\n",
       " [('It', 'appeal', 'nsubj'),\n",
       "  (\"DIDN'T\", 'appeal', 'intj'),\n",
       "  ('appeal', 'appeal', 'ROOT'),\n",
       "  ('to', 'appeal', 'prep'),\n",
       "  ('me', 'to', 'pobj'),\n",
       "  ('.', 'appeal', 'punct')],\n",
       " [('I', 'preferred', 'nsubj'),\n",
       "  ('preferred', 'preferred', 'ROOT'),\n",
       "  ('Los', 'Angeles', 'compound'),\n",
       "  ('Angeles', 'preferred', 'dobj'),\n",
       "  ('.', 'preferred', 'punct')]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(c.text, c.head.text, c.dep_) for c in nlp(sentence.text)]\n",
    "\n",
    " for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition** is the process of classifying words in a sentence based on its noun category (PERSON, FACILITY, ORGANIZATION, GEOPOLITICAL ENTITY, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[(entity.text, entity.label_) for entity in nlp(sentence.text).ents] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2', 'CARDINAL'), ('New York', 'GPE'), ('2011', 'DATE')], [], [('Los Angeles', 'GPE')]]\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply the 7 different methods to preprocess o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìï Hovy, D. (2020). Text Analysis in Python for Social Scientists: Discovery and Exploration. Cambridge University Press.\n",
    "\n",
    "üåç https://medium.com/zero-equals-false/one-hot-encoding-129ccc293cda\n",
    "\n",
    "üåç https://markroxor.github.io/gensim/static/notebooks/word2vec.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
