{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to the basics of importing texts. \n",
    "You'll learn about the data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tools for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because text analysis techniques are primarily applied machine learning, a language that has rich scientific and numeric computing libraries is necessary. When it comes to tools for performing machine learning on text, Python has a powerhouse suite that includes NLTK, Gensim, and spaCy:\n",
    "    \n",
    "- **NLTK**, the Natural Language Tool-Kit, is a ‚Äúbatteries included‚Äù resource for NLP written in Python by experts in academia. Originally a pedagogical tool for teach‚Äê\n",
    "ing NLP, it contains corpora, lexical resources, grammars, language processing algorithms, and pretrained models that allow Python programmers to quickly get started processing text data in a variety of languages. üëâ https://www.nltk.org/\n",
    "\n",
    "- **Gensim** is a robust, efficient, and hassle-free library that focuses on unsupervised semantic modeling of text. Originally designed to find similarity between docu‚Äê\n",
    "ments (generate similarity), it now exposes topic modeling methods for latent semantic techniques, and includes other unsupervised libraries such as word2vec. üëâ https://radimrehurek.com/gensim/\n",
    "\n",
    "- **spaCy** provides production-grade language processing by implementing the academic state-of-the-art into a simple and easy-to-use API. In particular, spaCy focuses on preprocessing text for deep learning or to build information extraction or natural language understanding systems on large volumes of text. üëâ https://spacy.io/\n",
    "\n",
    "üìï Bengfort, B., Bilbro, R., & Ojeda, T. (2018). *Applied text analysis with python: Enabling language-aware data products with machine learning.* O'Reilly Media, Inc.\n",
    "\n",
    "üåç https://course.spacy.io/en/chapter1\n",
    "\n",
    "üåç https://www.nltk.org/book/ch02.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 . Introduction to Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a variable in English call *nlp*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy \n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you process a text with the nlp object, spaCy creates a Doc object ‚Äì short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù **Your turn:**\n",
    "\n",
    "Try ot some of the 55+ available languages: https://spacy.io/usage/models#languages.\n",
    "\n",
    "- Import the <tt> language <tt> class from <tt> spacy.lang.en <tt> and create a new <tt> mlp <tt>  object.\n",
    "- Create a <tt> doc <tt> and print its text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language class\n",
    "from spacy.lang.____ import ____ \n",
    "\n",
    "# Create the nlp object\n",
    "nlp = ____\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Write here your sentence on your language.\")\n",
    "\n",
    "# Print the document text\n",
    "print(____.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 . Introduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/avaldivia/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "# Download Gutenberg package\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick out the first of these texts ‚Äî Emma by Jane Austen ‚Äî and give it a short name, emma, then find out how many words it contains:\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
