{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Live coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show you how we analyse a text in real life. To do so, we will examine two judge responses to asylum's claims in the UK.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend of symbols:\n",
    "\n",
    "- ü§ì: Tips\n",
    "\n",
    "- ü§ñüìù: Your turn\n",
    "\n",
    "- ‚ùì: Question\n",
    "\n",
    "- üí´: Extra exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Read text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned in this course, the first step is to import the text into this notebook.\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "- 1) Copy and paste content in a **<tt>.txt<tt>** file.\n",
    "- 2) Install **<tt>pdftotext<tt>**: https://github.com/jalan/pdftotext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "# Read the raw file from txt\n",
    "f = open('../data/asylum_claims.txt','r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's substitute \\n by spaces\n",
    "import re\n",
    "text = re.sub('\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "\n",
    "# Load your PDF\n",
    "with open(\"../data/PA059452018.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "\n",
    "# If it's password-protected\n",
    "with open(\"../data/PA002402019.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f, \"secret\")\n",
    "\n",
    "# How many pages?\n",
    "print(len(pdf))\n",
    "\n",
    "# Iterate over all the pages\n",
    "for page in pdf:\n",
    "    print(page)\n",
    "\n",
    "# Read some individual pages\n",
    "print(pdf[0])\n",
    "print(pdf[1])\n",
    "\n",
    "# Read all the text into one string\n",
    "pdf = \"\\n\\n\".join(pdf)\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì It is important when analysing text to know the basic figures: \n",
    "- How many words do we have? \n",
    "- How many sentences? \n",
    "- What are the most common words? \n",
    "\n",
    "\n",
    "‚ùì More questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. How many words do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words do we have?\n",
    "words_txt = text.split()\n",
    "len(words_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pdf = pdf.split()\n",
    "len(words_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. How many sentences do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_txt = text.split('.')\n",
    "len(sent_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_pdf = pdf.split('.')\n",
    "len(sent_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_txt_nltk = sent_tokenize(text)\n",
    "print(len(sent_txt_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_pdf_nltk = sent_tokenize(pdf)\n",
    "print(len(sent_pdf_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. What are the most common words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words? \n",
    "wordfreq_txt = []\n",
    "\n",
    "# count words in text\n",
    "for w in words_txt:\n",
    "    wordfreq_txt.append(words_txt.count(w))\n",
    "    \n",
    "# create a list with words and its frequency\n",
    "word_list = list(set(zip(words_txt, wordfreq_txt)))\n",
    "\n",
    "    \n",
    "print(\"Pairs\\n\" + str(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words? \n",
    "wordfreq_pdf = []\n",
    "\n",
    "# count words in text\n",
    "for w in words_pdf:\n",
    "    wordfreq_pdf.append(words_pdf.count(w))\n",
    "    \n",
    "# create a list with words and its frequency\n",
    "word_list = list(set(zip(words_pdf, wordfreq_pdf)))\n",
    "\n",
    "# function to sort the list by second item of tuple\n",
    "def sort_pairs(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))  \n",
    "\n",
    "word_list_sort = sort_pairs(word_list)\n",
    "    \n",
    "print(\"Pairs\\n\" + str(word_list_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(word_list_sort)\n",
    "df.columns = ['words', 'counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df.loc[0:10,:], x='counts', y='words', orientation='h', text = 'words',\n",
    "             labels={\n",
    "                     \"counts\": \"Frequency\",\n",
    "                     \"words\": \"Words\"\n",
    "                 },)\n",
    "fig.layout.yaxis.type = 'category'\n",
    "fig.update_layout(yaxis_categoryorder = 'total ascending')\n",
    "fig.update_layout(yaxis=dict(showticklabels=False))\n",
    "fig.update_traces(texttemplate='%{text}', textposition='auto', marker_color='green')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',   title={\n",
    "        'text': \"Words Frequency in Tribunal Appeals\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Does that give information of the content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the text with some  techniques we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove capital letters\n",
    "text_clean = ' '.join(w.lower() for w in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = ' '.join(w for w in text_clean.split() if w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncutaction symbols\n",
    "text_clean = [[token.lemma_ for token in sentence] for sentence in nlp(text_clean).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4. Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text_clean_counter = dict(Counter(text_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame.from_dict(text_clean_counter, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.reset_index(level=0, inplace=True)\n",
    "df_clean.columns = ['words', 'counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_clean.loc[0:10,:], x='counts', y='words', orientation='h', text = 'words',\n",
    "             labels={\n",
    "                     \"counts\": \"Frequency\",\n",
    "                     \"words\": \"Words\"\n",
    "                 },)\n",
    "fig.layout.yaxis.type = 'category'\n",
    "fig.update_layout(yaxis_categoryorder = 'total ascending')\n",
    "fig.update_layout(yaxis=dict(showticklabels=False))\n",
    "fig.update_traces(texttemplate='%{text}', textposition='auto', marker_color='purple')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',   title={\n",
    "        'text': \"Words Frequency in Tribunal Appeals\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's show the word cloud of this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "word_cloud = WordCloud(background_color=\"white\", repeat=True)\n",
    "word_cloud.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. What we have learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù Now it's your turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Find the word 'EURODAC' using the function **<tt>search<tt>** from the **<tt>re<tt>** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Create a word cloud with different colour pattern using the text from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Remove symbols from **<tt>df_clean<tt>** and plot again the frequency of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
