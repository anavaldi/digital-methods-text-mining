{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to the basics of analysing words. \n",
    "You'll learn how to preprocess and represent words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend of symbols:\n",
    "\n",
    "- ü§ì: Tips\n",
    "\n",
    "- ü§ñüìù: Your turn\n",
    "\n",
    "- ‚ùì: Question\n",
    "\n",
    "- üí´: Extra exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll learn how to transform words into vectors. Let's start with one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library **<tt> sklearn <tt>** has a function that transforms categorical features to one-hot vectors:\n",
    "    \n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can i eat the pizza'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can eat the pizza\n"
     ]
    }
   ],
   "source": [
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'i', 'eat', 'the', 'pizza']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_array = array(doc1)\n",
    "doc2_array = array(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['can', 'i', 'eat', 'the', 'pizza'], dtype='<U5')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = doc1+doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'i', 'eat', 'the', 'pizza', 'you', 'can', 'eat', 'the', 'pizza']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can' 'i' 'eat' 'the' 'pizza' 'you' 'can' 'eat' 'the' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "values = array(data)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code transforms string sentences into a list and an array of words that we can manipulate later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will transform words to numbers based on its position. To do so, we will use the **<tt> LabelEncoder() <tt>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 4 3 5 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this variable integer encoded as a matrix, we could say that in contains 1 row and 10 columns (1x10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the reshape method, we will transpose the integer encoded array into a matrix of 10 rows and 1 column (10 x 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [4],\n",
       "       [3],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [4],\n",
       "       [3]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'eat', 'i', 'pizza', 'the', 'you']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows the order of words in the matrix\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the news dataset and calculate the onehot encoding of the first new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('../data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>media</th>\n",
       "      <th>corpus</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatic</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>The reindeer is the emblematic Christmas anima...</td>\n",
       "      <td>Weatherwatch: reindeer adapted to snow but not...</td>\n",
       "      <td>https://www.theguardian.com/world/2019/dec/23/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatic</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>The European parliament is split over whether ...</td>\n",
       "      <td>European parliament split on declaring climate...</td>\n",
       "      <td>https://www.theguardian.com/world/2019/nov/26/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climatic</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Fisayo Soyombo was eating an evening snack in ...</td>\n",
       "      <td>‚ÄòClimate of fear‚Äô: Nigeria intensifies crackdo...</td>\n",
       "      <td>https://www.theguardian.com/world/2019/nov/14/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climatic</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>The European Union considers itself as a leade...</td>\n",
       "      <td>EU's soaring climate rhetoric not always match...</td>\n",
       "      <td>https://www.theguardian.com/world/2019/dec/11/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>climatic</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Good morning, we‚Äôre now exactly two weeks out ...</td>\n",
       "      <td>Thursday briefing: Political climate too hot f...</td>\n",
       "      <td>https://www.theguardian.com/world/2019/nov/28/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic         media                                             corpus  \\\n",
       "0  climatic  The Guardian  The reindeer is the emblematic Christmas anima...   \n",
       "1  climatic  The Guardian  The European parliament is split over whether ...   \n",
       "2  climatic  The Guardian  Fisayo Soyombo was eating an evening snack in ...   \n",
       "3  climatic  The Guardian  The European Union considers itself as a leade...   \n",
       "4  climatic  The Guardian  Good morning, we‚Äôre now exactly two weeks out ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Weatherwatch: reindeer adapted to snow but not...   \n",
       "1  European parliament split on declaring climate...   \n",
       "2  ‚ÄòClimate of fear‚Äô: Nigeria intensifies crackdo...   \n",
       "3  EU's soaring climate rhetoric not always match...   \n",
       "4  Thursday briefing: Political climate too hot f...   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.theguardian.com/world/2019/dec/23/...  \n",
       "1  https://www.theguardian.com/world/2019/nov/26/...  \n",
       "2  https://www.theguardian.com/world/2019/nov/14/...  \n",
       "3  https://www.theguardian.com/world/2019/dec/11/...  \n",
       "4  https://www.theguardian.com/world/2019/nov/28/...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_new = df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reindeer is the emblematic Christmas animal and, while not exactly magical, it is among the best adapted to snowy conditions.For a start, a reindeer‚Äôs feet have four toes with dewclaws that spread out to distribute its weight like snowshoes, and are equipped with sharp hooves for digging in snow.A reindeer‚Äôs nose warms the air on its way to the lungs, cooling it again before it is exhaled. As well as retaining heat, this helps prevent water from being lost as vapour. This is why reindeer breath does not steam like human and horse breath.A reindeer‚Äôs thick double-layered coat is so efficient that it is more likely to overheat than get too cold, especially when running. When this happens, reindeer pant like dogs to cool down, bypassing the nasal heat exchanger.Snowfields may be featureless to human eyes, but reindeer are sensitive to ultraviolet light, an evolutionary development that only occurred after the animals moved to Arctic regions. Snow reflects ultraviolet, so this ultravision allows reindeer to spot anything lying on it, in particular lichen, which they eat, and traces of urine showing where other reindeer have passed.But while reindeer thrive in Christmas-card weather, they are increasingly challenged by climate change and the freeze-thaw conditions that produce poor grazing.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_new_low = first_new.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the reindeer is the emblematic christmas animal and, while not exactly magical, it is among the best adapted to snowy conditions.for a start, a reindeer‚Äôs feet have four toes with dewclaws that spread out to distribute its weight like snowshoes, and are equipped with sharp hooves for digging in snow.a reindeer‚Äôs nose warms the air on its way to the lungs, cooling it again before it is exhaled. as well as retaining heat, this helps prevent water from being lost as vapour. this is why reindeer breath does not steam like human and horse breath.a reindeer‚Äôs thick double-layered coat is so efficient that it is more likely to overheat than get too cold, especially when running. when this happens, reindeer pant like dogs to cool down, bypassing the nasal heat exchanger.snowfields may be featureless to human eyes, but reindeer are sensitive to ultraviolet light, an evolutionary development that only occurred after the animals moved to arctic regions. snow reflects ultraviolet, so this ultravision allows reindeer to spot anything lying on it, in particular lichen, which they eat, and traces of urine showing where other reindeer have passed.but while reindeer thrive in christmas-card weather, they are increasingly challenged by climate change and the freeze-thaw conditions that produce poor grazing.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_new_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_new_low_list = first_new_low.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'reindeer',\n",
       " 'is',\n",
       " 'the',\n",
       " 'emblematic',\n",
       " 'christmas',\n",
       " 'animal',\n",
       " 'and,',\n",
       " 'while',\n",
       " 'not',\n",
       " 'exactly',\n",
       " 'magical,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'among',\n",
       " 'the',\n",
       " 'best',\n",
       " 'adapted',\n",
       " 'to',\n",
       " 'snowy',\n",
       " 'conditions.for',\n",
       " 'a',\n",
       " 'start,',\n",
       " 'a',\n",
       " 'reindeer‚Äôs',\n",
       " 'feet',\n",
       " 'have',\n",
       " 'four',\n",
       " 'toes',\n",
       " 'with',\n",
       " 'dewclaws',\n",
       " 'that',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'to',\n",
       " 'distribute',\n",
       " 'its',\n",
       " 'weight',\n",
       " 'like',\n",
       " 'snowshoes,',\n",
       " 'and',\n",
       " 'are',\n",
       " 'equipped',\n",
       " 'with',\n",
       " 'sharp',\n",
       " 'hooves',\n",
       " 'for',\n",
       " 'digging',\n",
       " 'in',\n",
       " 'snow.a',\n",
       " 'reindeer‚Äôs',\n",
       " 'nose',\n",
       " 'warms',\n",
       " 'the',\n",
       " 'air',\n",
       " 'on',\n",
       " 'its',\n",
       " 'way',\n",
       " 'to',\n",
       " 'the',\n",
       " 'lungs,',\n",
       " 'cooling',\n",
       " 'it',\n",
       " 'again',\n",
       " 'before',\n",
       " 'it',\n",
       " 'is',\n",
       " 'exhaled.',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'retaining',\n",
       " 'heat,',\n",
       " 'this',\n",
       " 'helps',\n",
       " 'prevent',\n",
       " 'water',\n",
       " 'from',\n",
       " 'being',\n",
       " 'lost',\n",
       " 'as',\n",
       " 'vapour.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'why',\n",
       " 'reindeer',\n",
       " 'breath',\n",
       " 'does',\n",
       " 'not',\n",
       " 'steam',\n",
       " 'like',\n",
       " 'human',\n",
       " 'and',\n",
       " 'horse',\n",
       " 'breath.a',\n",
       " 'reindeer‚Äôs',\n",
       " 'thick',\n",
       " 'double-layered',\n",
       " 'coat',\n",
       " 'is',\n",
       " 'so',\n",
       " 'efficient',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'more',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'overheat',\n",
       " 'than',\n",
       " 'get',\n",
       " 'too',\n",
       " 'cold,',\n",
       " 'especially',\n",
       " 'when',\n",
       " 'running.',\n",
       " 'when',\n",
       " 'this',\n",
       " 'happens,',\n",
       " 'reindeer',\n",
       " 'pant',\n",
       " 'like',\n",
       " 'dogs',\n",
       " 'to',\n",
       " 'cool',\n",
       " 'down,',\n",
       " 'bypassing',\n",
       " 'the',\n",
       " 'nasal',\n",
       " 'heat',\n",
       " 'exchanger.snowfields',\n",
       " 'may',\n",
       " 'be',\n",
       " 'featureless',\n",
       " 'to',\n",
       " 'human',\n",
       " 'eyes,',\n",
       " 'but',\n",
       " 'reindeer',\n",
       " 'are',\n",
       " 'sensitive',\n",
       " 'to',\n",
       " 'ultraviolet',\n",
       " 'light,',\n",
       " 'an',\n",
       " 'evolutionary',\n",
       " 'development',\n",
       " 'that',\n",
       " 'only',\n",
       " 'occurred',\n",
       " 'after',\n",
       " 'the',\n",
       " 'animals',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'arctic',\n",
       " 'regions.',\n",
       " 'snow',\n",
       " 'reflects',\n",
       " 'ultraviolet,',\n",
       " 'so',\n",
       " 'this',\n",
       " 'ultravision',\n",
       " 'allows',\n",
       " 'reindeer',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'anything',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'it,',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'lichen,',\n",
       " 'which',\n",
       " 'they',\n",
       " 'eat,',\n",
       " 'and',\n",
       " 'traces',\n",
       " 'of',\n",
       " 'urine',\n",
       " 'showing',\n",
       " 'where',\n",
       " 'other',\n",
       " 'reindeer',\n",
       " 'have',\n",
       " 'passed.but',\n",
       " 'while',\n",
       " 'reindeer',\n",
       " 'thrive',\n",
       " 'in',\n",
       " 'christmas-card',\n",
       " 'weather,',\n",
       " 'they',\n",
       " 'are',\n",
       " 'increasingly',\n",
       " 'challenged',\n",
       " 'by',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'and',\n",
       " 'the',\n",
       " 'freeze-thaw',\n",
       " 'conditions',\n",
       " 'that',\n",
       " 'produce',\n",
       " 'poor',\n",
       " 'grazing.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_new_low_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_new_low_list_array = array(first_new_low_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'reindeer', 'is', 'the', 'emblematic', 'christmas',\n",
       "       'animal', 'and,', 'while', 'not', 'exactly', 'magical,', 'it',\n",
       "       'is', 'among', 'the', 'best', 'adapted', 'to', 'snowy',\n",
       "       'conditions.for', 'a', 'start,', 'a', 'reindeer‚Äôs', 'feet', 'have',\n",
       "       'four', 'toes', 'with', 'dewclaws', 'that', 'spread', 'out', 'to',\n",
       "       'distribute', 'its', 'weight', 'like', 'snowshoes,', 'and', 'are',\n",
       "       'equipped', 'with', 'sharp', 'hooves', 'for', 'digging', 'in',\n",
       "       'snow.a', 'reindeer‚Äôs', 'nose', 'warms', 'the', 'air', 'on', 'its',\n",
       "       'way', 'to', 'the', 'lungs,', 'cooling', 'it', 'again', 'before',\n",
       "       'it', 'is', 'exhaled.', 'as', 'well', 'as', 'retaining', 'heat,',\n",
       "       'this', 'helps', 'prevent', 'water', 'from', 'being', 'lost', 'as',\n",
       "       'vapour.', 'this', 'is', 'why', 'reindeer', 'breath', 'does',\n",
       "       'not', 'steam', 'like', 'human', 'and', 'horse', 'breath.a',\n",
       "       'reindeer‚Äôs', 'thick', 'double-layered', 'coat', 'is', 'so',\n",
       "       'efficient', 'that', 'it', 'is', 'more', 'likely', 'to',\n",
       "       'overheat', 'than', 'get', 'too', 'cold,', 'especially', 'when',\n",
       "       'running.', 'when', 'this', 'happens,', 'reindeer', 'pant', 'like',\n",
       "       'dogs', 'to', 'cool', 'down,', 'bypassing', 'the', 'nasal', 'heat',\n",
       "       'exchanger.snowfields', 'may', 'be', 'featureless', 'to', 'human',\n",
       "       'eyes,', 'but', 'reindeer', 'are', 'sensitive', 'to',\n",
       "       'ultraviolet', 'light,', 'an', 'evolutionary', 'development',\n",
       "       'that', 'only', 'occurred', 'after', 'the', 'animals', 'moved',\n",
       "       'to', 'arctic', 'regions.', 'snow', 'reflects', 'ultraviolet,',\n",
       "       'so', 'this', 'ultravision', 'allows', 'reindeer', 'to', 'spot',\n",
       "       'anything', 'lying', 'on', 'it,', 'in', 'particular', 'lichen,',\n",
       "       'which', 'they', 'eat,', 'and', 'traces', 'of', 'urine', 'showing',\n",
       "       'where', 'other', 'reindeer', 'have', 'passed.but', 'while',\n",
       "       'reindeer', 'thrive', 'in', 'christmas-card', 'weather,', 'they',\n",
       "       'are', 'increasingly', 'challenged', 'by', 'climate', 'change',\n",
       "       'and', 'the', 'freeze-thaw', 'conditions', 'that', 'produce',\n",
       "       'poor', 'grazing.'], dtype='<U20')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_new_low_list_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 105  72 123  46  27  10   9 146  89  50  83  73  72   6 123  19   1\n",
      " 128 115  33   0 119   0 106  55  63  57 129 148  37 122 118  95 128  39\n",
      "  75 141  78 114   8  14  47 148 110  67  56  38  70 113 106  88 137 123\n",
      "   4  92  75 139 128 123  81  35  73   3  17  73  72  52  15 142  15 107\n",
      "  65 126  66 101 138  59  18  80  15 136 126  72 147 105  20  40  89 120\n",
      "  78  69   8  68  21 106 125  42  30  72 116  45 122  73  72  85  79 128\n",
      "  96 121  60 130  31  48 143 108 143 126  62 105  97  78  41 128  34  43\n",
      "  24 123  87  64  51  84  16  54 128  69  53  22 105  14 109 128 132  77\n",
      "   7  49  36 122  93  90   2 123  11  86 128  13 104 112 103 133 116 126\n",
      " 134   5 105 128 117  12  82  92  74  70  98  76 145 124  44   8 131  91\n",
      " 135 111 144  94 105  63  99 146 105 127  70  28 140 124  14  71  25  23\n",
      "  29  26   8 123  58  32 122 102 100  61]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(first_new_low_list_array)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123],\n",
       "       [105],\n",
       "       [ 72],\n",
       "       [123],\n",
       "       [ 46],\n",
       "       [ 27],\n",
       "       [ 10],\n",
       "       [  9],\n",
       "       [146],\n",
       "       [ 89],\n",
       "       [ 50],\n",
       "       [ 83],\n",
       "       [ 73],\n",
       "       [ 72],\n",
       "       [  6],\n",
       "       [123],\n",
       "       [ 19],\n",
       "       [  1],\n",
       "       [128],\n",
       "       [115],\n",
       "       [ 33],\n",
       "       [  0],\n",
       "       [119],\n",
       "       [  0],\n",
       "       [106],\n",
       "       [ 55],\n",
       "       [ 63],\n",
       "       [ 57],\n",
       "       [129],\n",
       "       [148],\n",
       "       [ 37],\n",
       "       [122],\n",
       "       [118],\n",
       "       [ 95],\n",
       "       [128],\n",
       "       [ 39],\n",
       "       [ 75],\n",
       "       [141],\n",
       "       [ 78],\n",
       "       [114],\n",
       "       [  8],\n",
       "       [ 14],\n",
       "       [ 47],\n",
       "       [148],\n",
       "       [110],\n",
       "       [ 67],\n",
       "       [ 56],\n",
       "       [ 38],\n",
       "       [ 70],\n",
       "       [113],\n",
       "       [106],\n",
       "       [ 88],\n",
       "       [137],\n",
       "       [123],\n",
       "       [  4],\n",
       "       [ 92],\n",
       "       [ 75],\n",
       "       [139],\n",
       "       [128],\n",
       "       [123],\n",
       "       [ 81],\n",
       "       [ 35],\n",
       "       [ 73],\n",
       "       [  3],\n",
       "       [ 17],\n",
       "       [ 73],\n",
       "       [ 72],\n",
       "       [ 52],\n",
       "       [ 15],\n",
       "       [142],\n",
       "       [ 15],\n",
       "       [107],\n",
       "       [ 65],\n",
       "       [126],\n",
       "       [ 66],\n",
       "       [101],\n",
       "       [138],\n",
       "       [ 59],\n",
       "       [ 18],\n",
       "       [ 80],\n",
       "       [ 15],\n",
       "       [136],\n",
       "       [126],\n",
       "       [ 72],\n",
       "       [147],\n",
       "       [105],\n",
       "       [ 20],\n",
       "       [ 40],\n",
       "       [ 89],\n",
       "       [120],\n",
       "       [ 78],\n",
       "       [ 69],\n",
       "       [  8],\n",
       "       [ 68],\n",
       "       [ 21],\n",
       "       [106],\n",
       "       [125],\n",
       "       [ 42],\n",
       "       [ 30],\n",
       "       [ 72],\n",
       "       [116],\n",
       "       [ 45],\n",
       "       [122],\n",
       "       [ 73],\n",
       "       [ 72],\n",
       "       [ 85],\n",
       "       [ 79],\n",
       "       [128],\n",
       "       [ 96],\n",
       "       [121],\n",
       "       [ 60],\n",
       "       [130],\n",
       "       [ 31],\n",
       "       [ 48],\n",
       "       [143],\n",
       "       [108],\n",
       "       [143],\n",
       "       [126],\n",
       "       [ 62],\n",
       "       [105],\n",
       "       [ 97],\n",
       "       [ 78],\n",
       "       [ 41],\n",
       "       [128],\n",
       "       [ 34],\n",
       "       [ 43],\n",
       "       [ 24],\n",
       "       [123],\n",
       "       [ 87],\n",
       "       [ 64],\n",
       "       [ 51],\n",
       "       [ 84],\n",
       "       [ 16],\n",
       "       [ 54],\n",
       "       [128],\n",
       "       [ 69],\n",
       "       [ 53],\n",
       "       [ 22],\n",
       "       [105],\n",
       "       [ 14],\n",
       "       [109],\n",
       "       [128],\n",
       "       [132],\n",
       "       [ 77],\n",
       "       [  7],\n",
       "       [ 49],\n",
       "       [ 36],\n",
       "       [122],\n",
       "       [ 93],\n",
       "       [ 90],\n",
       "       [  2],\n",
       "       [123],\n",
       "       [ 11],\n",
       "       [ 86],\n",
       "       [128],\n",
       "       [ 13],\n",
       "       [104],\n",
       "       [112],\n",
       "       [103],\n",
       "       [133],\n",
       "       [116],\n",
       "       [126],\n",
       "       [134],\n",
       "       [  5],\n",
       "       [105],\n",
       "       [128],\n",
       "       [117],\n",
       "       [ 12],\n",
       "       [ 82],\n",
       "       [ 92],\n",
       "       [ 74],\n",
       "       [ 70],\n",
       "       [ 98],\n",
       "       [ 76],\n",
       "       [145],\n",
       "       [124],\n",
       "       [ 44],\n",
       "       [  8],\n",
       "       [131],\n",
       "       [ 91],\n",
       "       [135],\n",
       "       [111],\n",
       "       [144],\n",
       "       [ 94],\n",
       "       [105],\n",
       "       [ 63],\n",
       "       [ 99],\n",
       "       [146],\n",
       "       [105],\n",
       "       [127],\n",
       "       [ 70],\n",
       "       [ 28],\n",
       "       [140],\n",
       "       [124],\n",
       "       [ 14],\n",
       "       [ 71],\n",
       "       [ 25],\n",
       "       [ 23],\n",
       "       [ 29],\n",
       "       [ 26],\n",
       "       [  8],\n",
       "       [123],\n",
       "       [ 58],\n",
       "       [ 32],\n",
       "       [122],\n",
       "       [102],\n",
       "       [100],\n",
       "       [ 61]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'adapted',\n",
       " 'after',\n",
       " 'again',\n",
       " 'air',\n",
       " 'allows',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'and,',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'anything',\n",
       " 'arctic',\n",
       " 'are',\n",
       " 'as',\n",
       " 'be',\n",
       " 'before',\n",
       " 'being',\n",
       " 'best',\n",
       " 'breath',\n",
       " 'breath.a',\n",
       " 'but',\n",
       " 'by',\n",
       " 'bypassing',\n",
       " 'challenged',\n",
       " 'change',\n",
       " 'christmas',\n",
       " 'christmas-card',\n",
       " 'climate',\n",
       " 'coat',\n",
       " 'cold,',\n",
       " 'conditions',\n",
       " 'conditions.for',\n",
       " 'cool',\n",
       " 'cooling',\n",
       " 'development',\n",
       " 'dewclaws',\n",
       " 'digging',\n",
       " 'distribute',\n",
       " 'does',\n",
       " 'dogs',\n",
       " 'double-layered',\n",
       " 'down,',\n",
       " 'eat,',\n",
       " 'efficient',\n",
       " 'emblematic',\n",
       " 'equipped',\n",
       " 'especially',\n",
       " 'evolutionary',\n",
       " 'exactly',\n",
       " 'exchanger.snowfields',\n",
       " 'exhaled.',\n",
       " 'eyes,',\n",
       " 'featureless',\n",
       " 'feet',\n",
       " 'for',\n",
       " 'four',\n",
       " 'freeze-thaw',\n",
       " 'from',\n",
       " 'get',\n",
       " 'grazing.',\n",
       " 'happens,',\n",
       " 'have',\n",
       " 'heat',\n",
       " 'heat,',\n",
       " 'helps',\n",
       " 'hooves',\n",
       " 'horse',\n",
       " 'human',\n",
       " 'in',\n",
       " 'increasingly',\n",
       " 'is',\n",
       " 'it',\n",
       " 'it,',\n",
       " 'its',\n",
       " 'lichen,',\n",
       " 'light,',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'lost',\n",
       " 'lungs,',\n",
       " 'lying',\n",
       " 'magical,',\n",
       " 'may',\n",
       " 'more',\n",
       " 'moved',\n",
       " 'nasal',\n",
       " 'nose',\n",
       " 'not',\n",
       " 'occurred',\n",
       " 'of',\n",
       " 'on',\n",
       " 'only',\n",
       " 'other',\n",
       " 'out',\n",
       " 'overheat',\n",
       " 'pant',\n",
       " 'particular',\n",
       " 'passed.but',\n",
       " 'poor',\n",
       " 'prevent',\n",
       " 'produce',\n",
       " 'reflects',\n",
       " 'regions.',\n",
       " 'reindeer',\n",
       " 'reindeer‚Äôs',\n",
       " 'retaining',\n",
       " 'running.',\n",
       " 'sensitive',\n",
       " 'sharp',\n",
       " 'showing',\n",
       " 'snow',\n",
       " 'snow.a',\n",
       " 'snowshoes,',\n",
       " 'snowy',\n",
       " 'so',\n",
       " 'spot',\n",
       " 'spread',\n",
       " 'start,',\n",
       " 'steam',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'this',\n",
       " 'thrive',\n",
       " 'to',\n",
       " 'toes',\n",
       " 'too',\n",
       " 'traces',\n",
       " 'ultraviolet',\n",
       " 'ultraviolet,',\n",
       " 'ultravision',\n",
       " 'urine',\n",
       " 'vapour.',\n",
       " 'warms',\n",
       " 'water',\n",
       " 'way',\n",
       " 'weather,',\n",
       " 'weight',\n",
       " 'well',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'why',\n",
       " 'with']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the number of unique words on that new?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì In that matrix (onehot_encoded), the number of columns represents the total number of **unique** words within the text, while the number of rows represents the total number of words (duplicated or not) within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the one-hot expression of *adapted*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì The one-hot encoding representation of adapted corresponds to the second column (position 1) of the one-hot encoding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding of 'arctic'\n",
    "onehot_encoded[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì Finally, if we sum the column or the one-hot encoding representation we will obtain the frequency of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(onehot_encoded[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Word embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a word2vec model in Python is scarily easy with **<tt> gensim <tt>**.\n",
    "    \n",
    "üåç https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the word2vec model taking a look to the parameters:\n",
    "\n",
    "üåç https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will prepare the data. To do so, we need to transform every sentence in a list within a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()\n",
    "\n",
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = [doc1, doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(doc3, size=300, window=3, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can analyse the vocabulary of this word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the embeddings by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['pizza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word2vec, we can analyse similarities across words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(negative=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And relations between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('pizza', 'eat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì Note that this model doesn't contain a lot of text, so it doesn't make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a word2vec embedding with the news corpus and extract the top 10 most similar words of *ultraviolet*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help to prepare the input for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a loop that iterates over the dataframe\n",
    "news_vec = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sent = row['corpus'].lower()\n",
    "    sent = sent.split()\n",
    "    news_vec.append(sent)    \n",
    " \n",
    "# Print the first element of the list:\n",
    "print(news_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí´ Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the most similiar word to *climate*.\n",
    "- Calculate the similarity between *climate* and *weather*.\n",
    "- Calculate the most similar word to *huamanitarian* + *climate* - *droguth*.\n",
    "\n",
    "Does make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Word preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate the examples we have seen previously in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of separate symbols by introducing extra white space is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/avaldivia/env37/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (47.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/avaldivia/env37/lib/python3.7/site-packages (from spacy) (1.19.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/avaldivia/env37/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/avaldivia/env37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/avaldivia/env37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/avaldivia/env37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/avaldivia/env37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/avaldivia/env37/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2b1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/avaldivia/env37/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"I've been 2 times to New York in 2011, but did not have the constitution for it. It DIDN'T appeal to me. I preferred Los Angeles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[token.text for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I',\n",
       "  \"'ve\",\n",
       "  'been',\n",
       "  '2',\n",
       "  'times',\n",
       "  'to',\n",
       "  'New',\n",
       "  'York',\n",
       "  'in',\n",
       "  '2011',\n",
       "  ',',\n",
       "  'but',\n",
       "  'did',\n",
       "  'not',\n",
       "  'have',\n",
       "  'the',\n",
       "  'constitution',\n",
       "  'for',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['It', \"DIDN'T\", 'appeal', 'to', 'me', '.'],\n",
       " ['I', 'preferred', 'Los', 'Angeles', '.']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', \"'ve\", 'been', '2', 'times', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.'], ['It', \"DIDN'T\", 'appeal', 'to', 'me', '.'], ['I', 'preferred', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its dictionary based (lemma) is called **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[token.lemma_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-PRON-', 'have', 'be', '2', 'time', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'do', 'not', 'have', 'the', 'constitution', 'for', '-PRON-', '.'], ['-PRON-', \"DIDN'T\", 'appeal', 'to', '-PRON-', '.'], ['-PRON-', 'prefer', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its stem is called **stemming**. \n",
    "\n",
    "This process is more radical than lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/avaldivia/env37/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /home/avaldivia/env37/lib/python3.7/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in /home/avaldivia/env37/lib/python3.7/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in /home/avaldivia/env37/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/avaldivia/env37/lib/python3.7/site-packages (from nltk) (0.15.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2b1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/avaldivia/env37/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stems = [[stemmer.stem(token) for token in sentence] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 've', 'been', '2', 'time', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitut', 'for', 'it', '.'], ['it', \"didn't\", 'appeal', 'to', 'me', '.'], ['i', 'prefer', 'los', 'angel', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech** corresponds to the process of classifying words to its category: nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [[token.pos_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRON', 'AUX', 'AUX', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'NUM', 'PUNCT', 'CCONJ', 'AUX', 'PART', 'AUX', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'PROPN', 'VERB', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'VERB', 'PROPN', 'PROPN', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords** is the process of removing words that cannot be beneficial for the analysis, like determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [[token.text for token in sentence if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV'} and not token.is_stop]\n",
    "for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['times', 'New', 'York', 'constitution'], [\"DIDN'T\", 'appeal'], ['preferred', 'Los', 'Angeles']]\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative using **<tt> nltk <tt>** is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sentiment analysis into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[token.sentiment for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing** is the process of classifying words in a sentence based on its syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[(c.text, c.head.text, c.dep_) for c in nlp(sentence.text)] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'been', 'nsubj'), (\"'ve\", 'been', 'aux'), ('been', 'been', 'ROOT'), ('2', 'been', 'attr'), ('times', '2', 'quantmod'), ('to', 'been', 'prep'), ('New', 'York', 'compound'), ('York', 'to', 'pobj'), ('in', 'been', 'prep'), ('2011', 'in', 'pobj'), (',', 'been', 'punct'), ('but', 'been', 'cc'), ('did', 'have', 'aux'), ('not', 'have', 'neg'), ('have', 'been', 'conj'), ('the', 'constitution', 'det'), ('constitution', 'have', 'dobj'), ('for', 'constitution', 'prep'), ('it', 'for', 'pobj'), ('.', 'been', 'punct')], [('It', 'appeal', 'nsubj'), (\"DIDN'T\", 'appeal', 'intj'), ('appeal', 'appeal', 'ROOT'), ('to', 'appeal', 'prep'), ('me', 'to', 'pobj'), ('.', 'appeal', 'punct')], [('I', 'preferred', 'nsubj'), ('preferred', 'preferred', 'ROOT'), ('Los', 'Angeles', 'compound'), ('Angeles', 'preferred', 'dobj'), ('.', 'preferred', 'punct')]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition** is the process of classifying words in a sentence based on its noun category (PERSON, FACILITY, ORGANIZATION, GEOPOLITICAL ENTITY, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[(entity.text, entity.label_) for entity in nlp(sentence.text).ents] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2', 'CARDINAL'), ('New York', 'GPE'), ('2011', 'DATE')], [], [('Los Angeles', 'GPE')]]\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the 7 different methods to preprocess words on the first row of the new's dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìï Hovy, D. (2020). Text Analysis in Python for Social Scientists: Discovery and Exploration. Cambridge University Press.\n",
    "\n",
    "üåç https://medium.com/zero-equals-false/one-hot-encoding-129ccc293cda\n",
    "\n",
    "üåç https://markroxor.github.io/gensim/static/notebooks/word2vec.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
